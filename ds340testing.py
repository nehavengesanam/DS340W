# -*- coding: utf-8 -*-
"""DS340TESTING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XTxoSaiOacyUxP3VZ70mbaiOwTPRaG6O
"""

!pip install transformers pandas scikit-learn

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("340dataset.csv")

# Filter samples into short (<50 words) and long (>200 words) length categories
df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))
short_texts = df[df['word_count'] < 50]
long_texts = df[df['word_count'] > 200]

print(f"Total samples: {len(df)}")
print(f"Short text samples: {len(short_texts)}")
print(f"Long text samples: {len(long_texts)}")

# Loading tokenizer and model
model_name = "roberta-base-openai-detector"  # Publicly accessible model for AI detection
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def detect_ai_generated(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        ai_prob = probs[0][1].item()  # Probability of being AI-generated
        return "AI-generated" if ai_prob > 0.5 else "Human-written", ai_prob

#short text results
short_text_results = []
for text in short_texts['text']:
    prediction, prob = detect_ai_generated(text)
    short_text_results.append({
        'text': text,
        'label': 'Short',
        'prediction': prediction,
        'probability': prob
    })

#long texts results
long_text_results = []
for text in long_texts['text']:
    prediction, prob = detect_ai_generated(text)
    long_text_results.append({
        'text': text,
        'label': 'Long',
        'prediction': prediction,
        'probability': prob
    })

# Convert results to DataFrames
short_df = pd.DataFrame(short_text_results)
long_df = pd.DataFrame(long_text_results)

df['true_label'] = df['label'].apply(lambda x: 1 if x == 'AI-generated' else 0)

# Accuracy and AUROC for short texts
short_df['true_label'] = short_texts['true_label'].values
short_df['predicted_label'] = short_df['prediction'].apply(lambda x: 1 if x == 'AI-generated' else 0)
short_accuracy = accuracy_score(short_df['true_label'], short_df['predicted_label'])
short_auroc = roc_auc_score(short_df['true_label'], short_df['probability'])

# Accuracy and AUROC for long texts
long_df['true_label'] = long_texts['true_label'].values
long_df['predicted_label'] = long_df['prediction'].apply(lambda x: 1 if x == 'AI-generated' else 0)
long_accuracy = accuracy_score(long_df['true_label'], long_df['predicted_label'])
long_auroc = roc_auc_score(long_df['true_label'], long_df['probability'])

print(f"Short Text Accuracy: {short_accuracy:.2f}, AUROC: {short_auroc:.2f}")
print(f"Long Text Accuracy: {long_accuracy:.2f}, AUROC: {long_auroc:.2f}")

def calculate_metrics_with_confusion_matrix(df):
    results = []
    for text in df['text']:
        prediction, prob = detect_ai_generated(text)
        results.append({
            'text': text,
            'true_label': df[df['text'] == text]['true_label'].values[0],
            'predicted_label': 1 if prediction == "AI-generated" else 0,
            'probability': prob,
        })

    results_df = pd.DataFrame(results)

    precision = precision_score(results_df['true_label'], results_df['predicted_label'])
    recall = recall_score(results_df['true_label'], results_df['predicted_label'])
    f1 = f1_score(results_df['true_label'], results_df['predicted_label'])

    # Confusion matrix
    cm = confusion_matrix(results_df['true_label'], results_df['predicted_label'])
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Human-written", "AI-generated"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

    return precision, recall, f1

precision, recall, f1 = calculate_metrics_with_confusion_matrix(df)

# Output metrics
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""TRAINING DATASET 120

TESTING DATASET

VALIDATION DATASET 40




"""
